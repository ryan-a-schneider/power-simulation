---
title: "Part 2"
output: html_document
---

# The Precision Approach to Power

Can be found [here](https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-ii/)

```{r packages, setup, include=FALSE}

pacman::p_load(tidyverse, brms, broom, report, easystats)
```

### Bayesian power analysis, Part 1 recap

This is how we simulated data for a power analysis to *detect an effect for null testing* in Part 1; i.e., when the only thing you care about is getting the bare minimum n to exclude a null value from the ROPE. Summary of steps is as follows:

1.  simulate a single, idealized sample and fit an initial model
2.  make a custom function that (a) generates simulated data, and (b) uses `update` to fit a model to it
3.  iterate over the simulated model x times

```{r}
#### STEP ONE: SIMULATE DATA AND FIT A SINGLE, INITIAL MODEL ####

# define the means
mu_c <- 0
mu_t <- 0.5

# determine the group size
n <- 50

# simulate the data
set.seed(1)

d <-
  tibble(group     = rep(c("control", "treatment"), each = n)) %>% # repeat each word 50 times
  mutate(treatment = ifelse(group == "control", 0, 1), # set the treatment condition variable coding (0= control, 1=treatment)
         y         = ifelse(group == "control", # create a new variable for the observed data....
                            rnorm(n, mean = mu_c, sd = 1), # for control group, set mu and sd
                            rnorm(n, mean = mu_t, sd = 1))) # set mu and sd for treatment group

# fit the model
fit <-
  brm(data = d,
      family = gaussian,
      y ~ 0 + Intercept + treatment,
      prior = c(prior(normal(0, 2), class = b),
                prior(student_t(3, 1, 1), class = sigma)),
      seed = 1)

# This was one simple practice run. It's a perfect example of creating a SINGLE data set of 50 observations
# But obviously we need to do this a lot more than once....
# Now to simulate many, many times. This requires some code tweaks to generate data iteratively...

#### STEP TWO: CREATE CUSTOM FUNCTION THAT BOTH SIMULATES DATA AND USES UPDATE() TO UPDATE THE INITIAL MODEL FIT ####

sim_d_and_fit <- function(seed, n) {
  
  mu_c <- 0
  mu_t <- 0.5
  
  set.seed(seed)
  
  d <-
    tibble(group     = rep(c("control", "treatment"), each = n)) %>% 
    mutate(treatment = ifelse(group == "control", 0, 1),
           y         = ifelse(group == "control", 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %>% 
    broom.mixed::tidy(prob = .95) %>% 
    filter(term == "treatment")
}

#### STEP THREE: ITERATE OVER n_sim= 100 TIMES ####

n_sim <- 100

s3 <-
  tibble(seed = 1:n_sim) %>% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 50)) %>% 
  unnest(tidy)

#### STEP FOUR: CHECK RESULTS ####

theme_set(theme_grey() +
            theme(panel.grid = element_blank()))

s3 %>% 
  ggplot(aes(x = seed, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  labs(x = "seed (i.e., simulation index)",
       y = expression(beta[1]))
```

The goal with this method is to find an n high enough so that in (e.g. 90% of simulations), 0 is not in your HDI. The min n to not get 0 in HDI 90% of the time.

But we're Bayesians. We can do better than this.

On to Part 2...

## Part 2: Estimating an effect with a minimum degree of precision.

Instead of looking at whether or not the interval contains the point Null, we can examine their widths instead. The wider the interval, the less precise the estimate.

Because this is a run of simulations, the HDI's won't all be the same...some will be wider and some narrower. Can examine the widths of the above chunk with this code:

```{r}
s3 %>%
  ggplot(aes(x = reorder(seed, conf.low), y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) +
  ylab(expression(beta[1])) +
  coord_cartesian(ylim = c(-.5, 1.3))
```

And compute their exact widths with this code:

```{r}
s3 <-
  s3 %>% 
  mutate(width = conf.high - conf.low)
```

Can even plot their widths in a histogram:

```{r}
s3 %>% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01)
```

The posterior shows that the most probable estimate for $\beta_1$ is about 0.6, and the intervals range from about [0.2, 1.0]. That's wide. Lots of uncertainty in that estimate.

So rather than settling and saying "welp, we know zero isn't in there so good enough", we can aim to make our most-probable-estimates more accurate.

Like, what if we could narrow the HDI from its current width of 0.8 to \< .7?

You can check how many widths are less than our goal of 0.7 with this code:

```{r}
s3 %>% 
  mutate(check = ifelse(width < .7, 1, 0)) %>% 
  summarise(`width power` = mean(check))
```

According to this, out of our 100 simulations, we have only a 0.08 probability of obtaining HDI's \<0.7, when our n=50 people. We can exclude zero at n=50, but we have a very uncertain and shitty estimate of the parameter still. So...onwards.

From here on out, we will *choose a minimum HDI width, and simulate to find the n necessary to make that happen.*

(stopping here)
