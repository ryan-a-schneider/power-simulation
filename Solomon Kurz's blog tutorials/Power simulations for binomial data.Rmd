---
title: 'Power, Part the Third: Simulating Binomial Data'
output: html_document
editor_options:
  chunk_output_type: console
---

This code was adapted from Solomon Kurz's tutorial, which can be found [here](https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-iii-b/).

```{r setup}
pacman::p_load(tidyverse,parameters,bayestestR, insight, rstanarm, parameters)
```


# Step 1: Generate idealized data from the DGP

Start with what you think would be the smallest reasonable effect (or the smallest effect in your study) that you'd want to detect.

```{r Generate data}

# Setting seed for reproducibility 
set.seed(91929)

# Setting sample size
n <- 10000

# Using rt() to drawing N log normally distributed values
d=tibble(y=rt(n, df = 7))

# Here's what these data look like in a histogram:

thematic::thematic_on(bg = "#222222", fg = "white", accent = "#0CE3AC")

d %>% 
  ggplot(aes(x = y)) + geom_histogram(fill="lightgreen", colour="white")+
  theme(panel.grid = element_blank())+
  coord_cartesian(x=c(-9,9))+
  labs(title = "Randomly drawn t density")
```


# Step 2: Model the data

*First, choose your prior(s).*

```{r}
library(brms)

get_prior(data = d, 
          family = student_t,
          y | trials(1) ~ 1)
```



*Then, feed prior into rstanarm and run the model.*

```{r}

fit2=stan_glm(y~1, 
              family = binomial(link = "logit"), 
              data=d, 
              prior = student_t(5,location=NULL, scale=NULL, autoscale = TRUE),
              prior_intercept = student_t(5,location = 0, scale = 2.5, autoscale = FALSE), 
              #prior_PD = FALSE, 
              algorithm = c("sampling"), 
              mean_PPD = TRUE,
              adapt_delta = 0.95, 
              chains=4,iter=2000,cores=4)
```


Check posterior diagnostics and parameter estimates.

```{r}
summarize_posterior=function(model){
  output=parameters::model_parameters(model) %>% select(c(Parameter,Median,CI_low,CI_high))
  output=as_tibble(output)
  return(output)
}

# parameter estimates
summarize_posterior(fit2)


#diagnostics
post_diag <- diagnostic_posterior(fit2)

effectsize::interpret_ess(post_diag$ESS) # effectsize V0.4.3.1
effectsize::interpret_rhat(post_diag$Rhat)
```


If you're using brms, be aware that **the intercept estimate is on the log-odds scale**; you can transform it with this code:

This is the *probability scale* (which I think is the same as the parameter), NOT the odds ratio scale
```{r}


```


But that's just the estimate for the parameter, we also need the HDI's.

Extract and plot posterior draws 

```{r}

get_parameters(fit2) %>% #extract posterior draws from model
  rename("Intercept"="(Intercept)") %>% # solving translation issues from brms to rstanarm
  transmute(p = inv_logit_scaled(Intercept)) %>% # transform from the log-odds to a probability metric
  # plot!
  ggplot(aes(x = p)) +
  geom_density(fill = "grey25", size = 0) +
  scale_x_continuous("probability of a hit", limits = c(0, 1)) +
  scale_y_continuous(NULL, breaks = NULL)

```

This shows that the posterior is very much below 0.5, so 0.5 is not a very credible value for this simulation.

We can also check this by looking at the posterior median and percentile-based 95% intervals
```{r}

head(posterior_samples(fit2)) # list of posterior draws. NOT HDI's! These are actual median estimates

test=as_tibble(posterior_samples(fit2) %>% 
                 rename("Intercept"="(Intercept)")) %>% 
  transmute(p = inv_logit_scaled(Intercept))

tidybayes::median_qi(test) # pull median quantiles


# all together now....
posterior_samples(fit2) %>%  # extract posterior draws from model
  rename("Intercept"="(Intercept)") %>% 
  transmute(p = inv_logit_scaled(Intercept)) %>% #convert to probability scale 
  tidybayes::median_qi() # check their quantiles

```

Check whether the true DGP parameter that was set above is within those intervals

# Mini power analysis

*On to iterative simulations. Now we want to know what happens when we generate 100 sets of data.*

**Use this function to generate the data**

```{r DGP function}

sim_data_fit_rstanarm <- function(seed, n_player) {
  
  n_trials <- 1
  prob_hit <- .25
  
  set.seed(seed)
  
  d <- tibble(y = rbinom(n    = n_player, 
                         size = n_trials, 
                         prob = prob_hit))
  
  update(fit2,
         data = d,
         seed = seed) %>% 
    posterior_samples() %>% 
    rename("Intercept"="(Intercept)") %>% # extract posterior draws
    transmute(p = inv_logit_scaled(Intercept)) %>%# convert back from log-odds scale
    tidybayes::median_qi() %>% #
    select(.lower:.upper)
  
}
```

RUN THE SIMULATION TO GENERATE DATA!

```{r mini power sim}
t1 <- Sys.time()

sim2 <-
  tibble(seed = 1:100) %>% 
  mutate(ci = map(seed, sim_data_fit_rstanarm, n_player = 50)) %>% 
  unnest()

t2 <- Sys.time()
totaltime_rstanarm=t2-t1
totaltime_rstanarm
```

Check results

```{r results}

# Plot HDI's
sim2 %>% 
  ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) +
  geom_hline(yintercept = c(.25, .5), color = "white") +
  geom_linerange() +
  xlab("seed (i.e., simulation index)") +
  scale_y_continuous("probability of hitting the ball", limits = c(0, 1))


#Summarize power
sim2 %>% 
  mutate(width = .upper - .lower) %>% 
  summarise(`conventional power` = mean(.upper < .5),
            `mean width`         = mean(width),
            `width below .25`    = mean(width < .25))
```


*When this starts to look like you want it to, follow this up by doing a proper simulation with >=1,000 iterations.*

As a final recommendation, he says you should "run your power simulation based on how you want to interpret and report your results." So, if we want to intepret and report primarily in terms of Odds Ratios, make sure that our estimates are on that scale when we do this.

# Running the full sim

```{r}

```

