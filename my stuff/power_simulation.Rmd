---
title: "Power Simulation"
author: "Ryan Schneider, M.A."
date: "2022-11-18"
output:
  html_document:
    theme: cerulean
    highlight: pygments
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: no
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
pacman::p_load(brms, easystats, tidyverse, janitor, flextable)
```

## I. Generate practice sample

Using 2C:35-7 Drug Free School Zone as an example. This ranges from 12 to 63 months, for row E, across all time points.

```{r generate_data}

# define the means and SD's
mu_struc=30
sd_struc=5

mu_UNstruc=36
sd_UNstruc=5
  
# determine the group size
n <- 50


# simulate the data
set.seed(1)

d_RS <-
  tibble(group     = rep(c("structured", "unstructured"), each = n)) %>% 
  mutate(treatment = ifelse(group == "structured", 0, 1),
         y         = ifelse(group == "structured", 
                                      rnorm(n, mean = mu_struc, sd = sd_struc), 
                                      rnorm(n, mean = mu_UNstruc, sd = sd_UNstruc)))
```

## II. Create practice model

```{r practice_run, results='hide'}

fit_lin=brm(data = d_RS, 
      family = gaussian,
      y ~ 1 + treatment,
      prior = c(prior(normal(30, 5), class = Intercept),
                prior(normal(0, 3), class = b),
                prior(lognormal(0, 5), class = sigma)),
      seed = 4,
      file = here::here("fitted models", "test_model"))

```

```{r}
describe_posterior(fit_lin)

plot(bayestestR::hdi(fit_lin, ci=.89))
```

## III. Simulate

Specify the number of simulations to be run, load the code, and then run the simulations.

```{r simulation, message=FALSE, results="hide"}

# A. Set the number of simulations
n_sim <- 1000


# B. Load function (substituting broom for easystats)
sim_d_and_fit_easystats <- function(seed, n) {
  
  set.seed(seed)
  
  d <-
    tibble(group     = rep(c("control", "treatment"), each = n)) |>  
    mutate(treatment = ifelse(group == "control", 0, 1),
           y         = ifelse(group == "control", 
                              rnorm(n, mean = mu_struc, sd = 1), 
                              rnorm(n, mean = mu_UNstruc, sd = 1))) 
  
  update(fit_lin,
         newdata = d, 
         seed = seed) |>  
    model_parameters("median", ci_method = "HDI", ci=.89) |> as_tibble() |> select(1, 3, 5:6) |>  
    filter(Parameter == "b_treatment")
}


# C. Run simulation
sim <-
  tibble(seed = 1:n_sim) |>  
  mutate(tidy = map(seed, sim_d_and_fit_easystats, n = 50)) |> 
  unnest(tidy)


# save(sim, file=here::here("Data", "large_sim.RData"), compress = FALSE)
```

## IV. Check Results

### Plots of HDI widths {.tabset}

Plot the interval widths, ordered by seed value.

```{r}
theme_set(theme_grey() +
            theme(panel.grid = element_blank()))


sim |>  
  ggplot(aes(x = seed, y = Median, ymin = CI_low, ymax = CI_high)) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  labs(x = "seed (i.e., simulation index)",
       y = expression(beta[1]))
```

Another plot of the HDI widths, this time ordered by the lower limit values.

> Notice how this arrangement highlights the differences in widths among the intervals. The wider the interval, the less precise the estimate.

```{r plots_1, fig.height=4, fig.width=8, dpi=300}


sim |> 
  ggplot(aes(x = reorder(seed, CI_low), y = Median, ymin = CI_low, ymax = CI_high)) +
  #geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete("reordered by the lower level of the intervals", breaks = NULL) +
  ylab(expression(beta[1])) # + coord_cartesian(ylim = c(-.5, 1.3))
```

### Quantifying width

> We might quantify those ranges by computing a `width` variable

```{r fig.height=5, fig.width=7, dpi=300, message=FALSE, warning=FALSE, results="hide"}
# compute exact HDI widths and summarize all 500 of them
sim <-
  sim |> 
  mutate(width = CI_high - CI_low)

# view HDI widths as a histogram
sim |> 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = NULL, color="green")+
  labs(title = "Frequency Histogram of Interval Widths", subtitle = "Across 1000 simulations")

```

And we can check the descriptives out in a neat table...

```{r table}

width_descriptives=sim |> summarize('Median width'=median(width),
                 'Mean width'=mean(width),
                 'Narrowest width'=min(width),
                 'Widest width'=max(width)) |> 
  mutate(across(everything(), round, 2))  


width_descriptives |> pivot_longer(cols = everything(),
                                   names_to = "HDI Summary Stat",
                                   values_to = "Value") |> 
  flextable::flextable() |> 
  flextable::autofit()
```

### Randomly sample an estimate

> Lets focus a bit and take a random sample from one of the simulation iterations.

```{r}
set.seed(1)

sim |> 
  sample_n(1) |>  
  mutate(seed = seed |> as.character()) |>  

  ggplot(aes(x = Median, xmin = CI_low, xmax = CI_high, y = seed)) +
  geom_vline(xintercept = c(0, .5), color = "white") +
  geom_pointrange() +
  labs(x = expression(beta[1]), y = "seed #") #+ xlim(0, 1)
```

The posterior shows that the most probable estimate for $\beta_1$ is about 6, and the 89% most-probable values do not include zero.

## Method 2: Targeting a Minimum Degree of Precision

### Example with simulation 1

> Though the posterior mean suggests the most probable value for β1β1​ is about 0.6, the intervals suggest values from about 0.2 to almost 1 are within the 95% probability range. That's a wide spread. Within psychology, a standardized mean difference of 0.2 would typically be considered small, whereas a difference of 1 would be large enough to raise a skeptical eyebrow or two.
>
> So instead of focusing on rejecting a null hypothesis like $μ_{control}=μ_{treatment}$​, we might instead use our simulation skills to determine the sample size we need to have most of our 95% intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation [AIPE; Maxwell et al. ([2008](https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-ii/#ref-maxwellSampleSizePlanning2008)); see also Kruschke ([2015](https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-ii/#ref-kruschkeDoingBayesianData2015))] approach to sample size planning.

> Thinking in terms of AIPE, in terms of precision, lets say we wanted widths of 0.7 or smaller.

This code shows the probability of achieving an interval width of (x desired size) or less, for a given sample size.

```{r}

target_max_width=.7

sim |> 
  mutate(check = ifelse(width < target_max_width, 1, 0)) |> 
  summarise(`width power` = mean(check))
```

> Once we're concerned with width size, about precision, the null hypothesis is no longer of direct relevance. An since we're no longer wed to thinking in terms of the null hypothesis, there's no real need to stick with a .8 threshold for evaluating 'width power.' Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analysis a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI wdiths and simulate to find the n necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger (1995) who suggested we shoot for an n that produces widths that pass that criterion on average.

Here is a demonstration and explanation of all three versions of AIPE criteria.

#### A. Kruschke's criterion and the minimum-threshold criterion

The difference between these two methods is that Kruschke recommends getting ALL the widths below the target; i.e., 100% of your HDI's are narrower than the max target you set. The minimum-threshold criterion is less strict, as you're targeting an acceptable proportion (e.g., 70 or 80%) of HDI's below some threshold.

First helpful to get a visual check to see if some of the widths are approaching the boundary.

```{r}

# check how the widths are distributed
sim |> 
  ggplot(aes(x=width)) + 
  geom_histogram(binwidth = NULL)
```

Now check the numbers.

```{r}

max_target_width=.7

sim |> 
  mutate(check=ifelse(width<max_target_width, 1, 0)) |> 
  summarize('proportion below target'=mean(check))
```

#### B. Mean-precision criterion

This is Joseph, Wolfson, and du Berger's (1995) suggestion to aim for an n that produces widths of a certain number, on average.

```{r}
sim_x |> summarize('average width'=mean(width))
```

### Simulation 2

Increase the sample size and re-run the simulation. The higher the sample size, the more narrowly the interval width should be around the true data-generating value.

```{r}

# change n_sim from above
n_sim=75
  
sim_2=tibble(seed = 1:n_sim) |>  
  mutate(b1 = map(seed, sim_d_and_fit_easystats, n = 50)) |> 
  unnest(b1) |> 
  mutate(width= CI_high - CI_low)

sim |> 
  mutate(check=ifelse(width<max_target_width, 1, 0)) |> 
  summarize('proportion below target'=mean(check),
            'average width'=mean(width))
```

## Packages and Session Info

```{r citations}
sessionInfo()
```
