---
title: "Power Simulation"
author: "Ryan Schneider, M.A."
date: "2022-11-18"
output:
  html_document:
    theme: cerulean
    highlight: pygments
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: no
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# load packages
pacman::p_load(brms, easystats, tidyverse)


# load functions

check_power_precision=function(sim_data, effect_direction, target_width, null_value){
  if(effect_direction=="negative") 
    return(sim_data |>
             mutate(check = ifelse(CI_high < null_value, 1, 0)) |> 
             summarize('Mean Width'         = round(mean(width),2),
                       'Width Power'    = round(mean(width < target_width),2),
                       'Null Detection Power'= round(mean(check),2)))
             
  
  if(effect_direction=="positive") 
    return(sim_data |> 
             mutate(check = ifelse(CI_low > null_value, 1, 0)) |>
             summarize('Mean Width'         = round(mean(width),2),
                       'Width Power'    = round(mean(width < target_width),2),
                       'Null Detection Power'= round(mean(check),2)))
  
}

check_precision=function(simulation_data, target_MAX_width){
  simulation_data |> 
    summarize('mean interval width'         = mean(width),
              'widths below target'    = mean(width < target_MAX_width))
}

```

## I. Generate practice sample

Step I is to create an sample of data that you think reflects the real-world differences between the conditions.

-   Using 2C:35-5 Manufacturing/Distributing a Controlled and Dangerous Substance (**1st degree**)
-   Penalty range for a pre-indictment plea ranges from 24 to 48 months (Row A). This represents the range the values of the DV could take.

```{r generate_data}

# define the means and SD's
mu_struc=36
sd_struc=10

mu_UNstruc=30
sd_UNstruc=10
  
# determine the group size
n <- 50


# simulate the data, making it reproducible
set.seed(1)

d1 <-
  tibble(group     = rep(c("structured", "unstructured"), each = n)) %>% 
  mutate(treatment = ifelse(group == "structured", 0, 1),
         y         = ifelse(group == "structured", 
                                      rnorm(n, mean = mu_struc, sd = sd_struc), 
                                      rnorm(n, mean = mu_UNstruc, sd = sd_UNstruc)))


# View the samples created
ggplot(d1, aes(x=y, fill=group)) + 
  theme_classic() +
  geom_density(alpha=.4)
```

## II. Create practice models

Now run linear regression to estimate the slope parameter in the sample above.

```{r practice_model_fit_check, results='hide', message=FALSE}

fit_lin=brm(data = d1,
            family = gaussian,
            formula = y ~ 1 + treatment,
            prior = c(prior(normal(33, 6), class = Intercept),
                      prior(student_t(5, 0, 9), class = b, coef="treatment"),
                      prior(lognormal(0, 5), class = sigma)),
            seed = 4,
            iter = 2500, chains = 2, cores = 2)


save(fit_lin, file=here::here("Models", "linear_power_fit.RData"))

#################### MODEL DIAGNOSTICS ##############################
diagnostic_posterior(fit_lin, diagnostic = c("ESS", "Rhat")) |> 
  mutate(Rhat_check=effectsize::interpret_rhat(Rhat),
         ESS_check=effectsize::interpret_ess(ESS))


#################### PARAMETERS AND EFFECT SIZE #################################
# check bayesian parameter estimates
describe_posterior(fit_lin, ci=.89, ci_method = "HDI",effects = "fixed",
                   rope_range = c(-1, 1))

# Compare to frequentist model
lm(y ~ treatment, data = d1) |> parameters::model_parameters()
```

```{r practice_model_info}

############ check ROPE ############################
# view plot
plot(bayestestR::rope(fit_lin, ci=.89, ci_method="HDI", range=c(-1, 1))) + 
  theme_classic() +
  see::scale_fill_material()

```

## III. Simulate

Specify the number of simulations to be run, load the function, and then run the simulations.

-   **NOTE! If you changed the way that the data is generated in section I, then you need to change the simulation function below to be consistent with it!**

```{r simulation, message=FALSE, warning=FALSE, results="hide"}

# A. Set the number of simulations and sample sizes
n_simulations=2000


# B. Load function
sim_d_and_fit_linear <- function(seed, n) {
  
  set.seed(seed)
  
  d <-
    tibble(group     = rep(c("control", "treatment"), each = n)) |>  
    mutate(treatment = ifelse(group == "control", 0, 1),
           y         = ifelse(group == "control", 
                              rnorm(n, mean = mu_struc, sd = sd_struc), 
                              rnorm(n, mean = mu_UNstruc, sd = sd_UNstruc))) 
  
  crap=capture.output(new_fit <- update(fit_lin,
                                        newdata = d, 
                                        seed = seed))
                      
  params=bayestestR::describe_posterior(new_fit, ci_method = "HDI", ci=.89) |> # HDI/CI settings
    select(c(Parameter, Median, CI_low, CI_high)) |> 
    mutate(across(c(Median, CI_low, CI_high), round, 2))
}

# B. Run simulation

sim1_linear_n50 <-
  tibble(seed = 1:n_simulations) |>  
  mutate(tidy = map(.x= seed, .f= sim_d_and_fit_linear, n = 50)) |> # adjust sample size as desired
  unnest(tidy) |> 
  mutate(width = CI_high - CI_low) |> # NOTE!!! This is the HDI; change function to get Credible Intervals
  filter(Parameter!="b_Intercept") # discard estimates of the intercept parameter

```

## IV. Check Results

### Method 1: Rejecting Null values

#### Proportion of intervals that contain a Null effect

Check the proportion of the simulations that have a lower interval limit that is greater than 0; i.e., the proportion of simulations where the HDI/CI does not include 0.

```{r}
sim1_linear_n50 |>  
  filter(Parameter == "b_treatment") |>  
  mutate(check = ifelse(CI_low > 2, 1, 0)) |>  
  summarise(power = mean(check))
```

### Method 2: Precision of Estimation

#### Quantifying width

```{r fig.height=5, fig.width=7, dpi=300, message=FALSE, warning=FALSE, results="hide"}

# view HDI widths as a histogram
sim1_linear_n50 |> 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = NULL, color="skyblue")+
  labs(title = "Frequency Histogram of Interval Widths")

```

And we can check the descriptives out in a neat table...

```{r table}

sim1_linear_n50 |> 
  summarize('Median width'=median(width),
            'Mean width'=mean(width),
            'Narrowest width'=min(width),
            'Widest width'=max(width)) |> 
  mutate(across(everything(), round, 2)) |> 
  pivot_longer(cols = everything(),
               names_to = "HDI Summary Stat",
               values_to = "Value") 
```

> Once we're concerned with width size, about precision, the null hypothesis is no longer of direct relevance. And since we're no longer wed to thinking in terms of the null hypothesis, there's no real need to stick with a .8 threshold for evaluating 'width power.' Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analysis a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI wdiths and simulate to find the n necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger (1995) who suggested we shoot for an n that produces widths that pass that criterion on average.

Here is a demonstration and explanation of all three versions of AIPE criteria.

#### A. Targeting a specific width

Even if your posterior suggests the difference between your groups is not zero, the Credible Interval or HDI might still be pretty wide on the size of the effect; so you could know *enough* to say "the effect isn't zero", but still be very uncertain about what it *actually is.* So...

> ...instead of focusing on rejecting a null hypothesis like $μ_{control}=μ_{treatment}$​, we might instead use our simulation skills to determine the sample size we need to have most of our intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation [AIPE; Maxwell et al. ([2008](https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-ii/#ref-maxwellSampleSizePlanning2008)); see also Kruschke ([2015](https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-ii/#ref-kruschkeDoingBayesianData2015))] approach to sample size planning.
>
> Thinking in terms of AIPE, in terms of precision, lets say we wanted widths of 0.7 or smaller.

This code shows the probability of achieving an interval width of (x desired size) or less, for a given sample size.

```{r}

target_max_width=.7

sim1_linear_n50 |> 
  mutate(check = ifelse(width < target_max_width, 1, 0)) |> 
  summarise(`width power` = mean(check))
```

#### B. Kruschke's minimum-threshold criterion

The difference between this and the above is that Kruschke recommends getting *100%* of the widths below the target. The general target approach above just aims for some arbitrarily acceptable proportion (e.g., 70 or 80%) of HDI's below the desired threshold. This approach ensures that all intervals will be precise.

First, it can be helpful to view the distribution of widths, to see if some of the widths are approaching the the "too wide" boundary.

```{r}

# visual check of the widths' distribution
sim1_linear_n50 |> 
  ggplot(aes(x=width)) + 
  geom_histogram(binwidth = NULL, color="red")

# checking the proportions
sim1_linear_n50 |> 
  mutate(check=ifelse(width<target_max_width, 1, 0)) |> 
  summarize('proportion below target'=mean(check))
```

#### C. Mean-precision criterion

This is Joseph, Wolfson, and du Berger's (1995) suggestion to aim for an n that produces widths of a certain number, on average.

```{r}
sim1_linear_n50 |> summarize('average width'=mean(width))
```

## V. More Simulations

If you did not achieve your goal(s) in the first simulation, increase the sample size and re-run it here. The higher the sample size, the more narrowly the interval width should be around the true data-generating value. Keep increasing the sample size and re-running the simulations in this section until you meet your goals.

```{r message=FALSE, warning=FALSE, results="hide"}

# Run more sims
# If you change the sample size, also change the name of the sim to make it easy to identify later

sim2_linear_n65 <-
  tibble(seed = 1:n_simulations) |>  
  mutate(tidy = map(.x= seed, .f= sim_d_and_fit_linear, n = 65)) |>
  unnest(tidy) |> 
  mutate(width = CI_high - CI_low) |> 
  filter(Parameter!="b_Intercept")


sim3_linear_n75 <-
  tibble(seed = 1:n_simulations) |>  
  mutate(tidy = map(.x= seed, .f= sim_d_and_fit_linear, n = 75)) |>
  unnest(tidy) |> 
  mutate(width = CI_high - CI_low) |> 
  filter(Parameter!="b_Intercept")


sim4_linear_n85 <-
  tibble(seed = 1:n_simulations) |>  
  mutate(tidy = map(.x= seed, .f= sim_d_and_fit_linear, n = 85)) |>
  unnest(tidy) |> 
  mutate(width = CI_high - CI_low) |> 
  filter(Parameter!="b_Intercept")


sim5_linear_n95 <-
  tibble(seed = 1:n_simulations) |>  
  mutate(tidy = map(.x= seed, .f= sim_d_and_fit_linear, n = 95)) |>
  unnest(tidy) |> 
  mutate(width = CI_high - CI_low) |> 
  filter(Parameter!="b_Intercept")


sim6_linear_n100 <-
  tibble(seed = 1:n_simulations) |>  
  mutate(tidy = map(.x= seed, .f= sim_d_and_fit_linear, n = 100)) |>
  unnest(tidy) |> 
  mutate(width = CI_high - CI_low) |> 
  filter(Parameter!="b_Intercept")



# compile all objects into a single list, with the sample size as the name for each level
linear_sims=list(n50=sim1_linear_n50, n65=sim2_linear_n65, n75=sim3_linear_n75, 
                 n85=sim4_linear_n85, n95=sim5_linear_n95) #, n100=sim6_linear_n100)

# save
save(linear_sims, file=here::here("Models", "linear_sims.RData"))

```

### Check Results

For the custom function:

-   `effect_direction` is the direction of the effect you created when you generated the idealized data in step one. Is there an increase or decrease? See your ROPE plot in Section II to see if the posterior is above or below 0.

-   `target_width` is for setting a goal for a minimum level of precision (cannot be empty, must supply a number).

-   `null_value` is the value to test against. Can either be 0 for a traditional Null Hypothesis test, or the upper or lower boundary of the ROPE, if doing an equivalence test. In the case of the latter, the effect direction determines the ROPE value (lower boundary if the direction is negative; the upper boundary if it's positive).

    -   see `bayestestR::rope_range()` for suggestions on default ROPE values for an unstandardized parameter

```{r}
# Method 1: Check one model at a time

sim2_linear_n65 |>  
  check_power_precision(effect_direction="negative", target_width = 5.7, null_value= -2)

##################### Method 2: DO FOR ALL AT ONCE with purrr ###############

# For i in list .x, apply function .f

linear_sims |> 
  # loop one: filter each list level for desired effect
  map(.f= filter, Parameter=="b_treatment") |>
  # loop two: apply custom function to each list level; return results as a unified table
  map_df(.f= check_power_precision, 
         effect_direction="negative", 
         target_width = 5.0, null_value= -1) |> 
  mutate(sample_size=names(linear_sims))
```

## Export table to MS Word

This table won't be completely done, but it'll be 95% of the way there. Finish the final touches in Word.

```{r}
########### Part 1: Prep a data frame ##############
pwr_table=linear_sims |> 
  map(.f= filter, Parameter=="b_treatment") |> # may be necessary if you have lots of effects
  map_df(.f= check_power_precision, effect_direction="negative", target_width = 5.0, null_value= -1.0) |> 
  mutate(sample_size=names(linear_sims)) |> 
  relocate(sample_size, .before = 1) |> 
  relocate("Null Detection Power", .after = "sample_size")

pwr_table=rename(pwr_table, "Sample Size"= "sample_size", "Mean HDI Width" = "Mean Width")


######### Part 2: Turn above tibble into a flextable ##################

library(flextable)
pwr_table= flextable(pwr_table)


######## Part 3: Apply formatting to flextable ########################

# create styling for the table's borders
border_styling=officer::fp_border(color = "black", style = "solid", width = 1)

# Apply formatting and styling
pwr_table=pwr_table |> 
    hline_bottom(border = border_styling, part = "header") |>
    hline_top(border = border_styling, part = "header") |>
    
    # CREATE A TITLE HEADER; APPLY FORMATTING
    add_header_lines(values = "Results of 2000 Power Simulations") |> 
    hline_top(border = fp_border_default(width = 0), part = "header") |> 
    flextable::align(align = "left", part = "header", i=1) |> 
    italic(part = "header", i=1) |> 
    
    # FIX BORDER IN TABLE BODY
    hline_bottom(border = border_styling, part = "body") |> 
    
    # SET FONT
    flextable::font(part = "all", fontname = "Times New Roman") |> 
    flextable::fontsize(part = "all", size = 11) |> 
    
    # SET COLUMN WIDTH/DIMENSIONS
    autofit(part = "header")  

    
pwr_table


######### Part 4: Add footer notes #######################################
pwr_table=add_footer_lines(pwr_table, 
                 values = "Note. Null Detection Power is the proportion of HDI's that did not fall within the ROPE; Width Power is the proportion of HDI's that were at least as narrow as the target threshold (of 5.0).") |> 
                 fontsize(part = "footer", size = 10) |>
                   font(part = "footer", fontname = "Times")


######################## EXPORT ###########################################
docx_file <- file.path(here::here("Figures and Tables"), "power_simulation_table.docx")

save_as_docx(pwr_table, path = docx_file)
```
